{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from Utils.api_ipc_request import construct_ipc_api_url\n",
    "from Utils.api_boundary_request import construct_boundary_api_url\n",
    "from Utils.temporal_coverage import plot_fewsnet_scenario_coverage\n",
    "from Utils.countries_to_process import select_country_codes\n",
    "from Utils.check_shapefile import pg_shapefile_exists\n",
    "from Utils.input_base_requirements import get_date_range, select_ipc_classification\n",
    "\n",
    "# Feature Engineering\n",
    "from Utils.filter_tables import desirable_attributes\n",
    "from Utils.merge_and_report import merge_ipc_with_boundaries, evaluate_merge_completness\n",
    "from Utils.scenario import define_scenario\n",
    "\n",
    "# Plot\n",
    "from Utils.plot_ipc_orig_boundaries import plot_historical_ipc\n",
    "\n",
    "# Load PG\n",
    "from Utils.give_PG_reference import provide_reference_frame\n",
    "\n",
    "\n",
    "#Compute area features\n",
    "from Utils.area_attributes import define_area_attributes\n",
    "\n",
    "\n",
    "#Perform Intersection\n",
    "from Utils.perform_intersection import intersect\n",
    "from Utils.build_envelope import envelope_buffer\n",
    "\n",
    "\n",
    "#Define process to conver FEWSNET to PG\n",
    "from Utils.select_process import define_process\n",
    "from Utils.user_process_selection import get_process_selection\n",
    "\n",
    "\n",
    "#Rejoin data to PG shapefile (GPD)\n",
    "from Utils.rejoin_pg_data import rejoin_to_pg\n",
    "\n",
    "from Utils.cumulative_population_attribution import engineer_population_attributes\n",
    "from Utils.pg_country_extent import create_country_geodataframe\n",
    "from Utils.leftjoin_to_pg_country import left_join_geodataframes\n",
    "from Utils.select_dates import get_dates_to_process\n",
    "\n",
    "# Save the data\n",
    "from Utils.csv_naming_conventions import apply_naming_convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "#### For process 6:\n",
    "\n",
    "1. Would you like to define specific population thresholds?\n",
    "If not, the default thresholds (50, 85 percentiles) will be used.\n",
    "\n",
    "- Define why and how this effects the process as a print() statement\n",
    "\n",
    "2. For the 90th percentile:\n",
    "Enter the population weight for the 90th percentile (e.g., 1.0): 1.0\n",
    "Enter the area weight for the 90th percentile (e.g., 0.0): 0.0\n",
    "\n",
    "- make sure that the two weights equal 1.0\n",
    "- if they do not equal 1.0 provide the user the opportunity to re-enter input values for both elements (restart this procedure / prompt)\n",
    "\n",
    "3. Loading shapefile from: /Users/gbenz/Documents/FEWSnet/Data/Processed/extent_shapefile/pg_viewser_extent.shp\n",
    "- print a line on what is happening, or ommit this print statement\n",
    "- I suspect this is referencing `result_gdf = rejoin_to_pg(result)` \n",
    "- but it may also be: `gpd_country_extent_df = create_country_geodataframe(country_name, year_int, shapefile_path=None)` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final csv result will be saved to path: /Users/gbenz/Documents/FEWSnet/Data/Processed/csv/FEWSnet_to_PG_\n"
     ]
    }
   ],
   "source": [
    "#project_root = Path(__file__).resolve()\n",
    "\n",
    "# Designate a local path:\n",
    "csv_path = '/Users/gbenz/Documents/FEWSnet/Data/Processed/csv/FEWSnet_to_PG_'\n",
    "print(f'The final csv result will be saved to path: {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to define historical data for ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PG reference shapefile has already been produced!\n",
      "Type 'q' at any time to quit the process.\n",
      "\n",
      "General date ranges associated with each IPC classification:\n",
      "1. IPC 2.0: ~2004–2012\n",
      "2. IPC 3.0: ~2013–2018\n",
      "3. IPC 3.1: ~2019–Present\n",
      "\n",
      "Please choose an IPC classification from the options above.\n",
      "Selected IPC classification: IPC 3.1 (IPC31)\n",
      "Please supply a start date in the YYYY-MM-DD format (e.g., 2004-01-01):\n",
      "Your selected start date is: 2020-01-01\n",
      "Please supply an end date in the YYYY-MM-DD format (e.g., 2012-12-31):\n",
      "Your selected end date is: 2024-01-01\n",
      "The API call constructed: https://fdw.fews.net/api/ipcphase.csv?start_date=2020-01-01&end_date=2024-01-01&classification_scale=IPC31\n",
      "\n",
      "Data downloaded and saved as ipc_data.csv to folder Data/Generated/ipc_data.csv\n",
      "\n",
      "['source_organization', 'source_document', 'country', 'country_code', 'geographic_group', 'fewsnet_region', 'geographic_unit_full_name', 'geographic_unit_name', 'unit_type', 'fnid', 'classification_scale', 'scenario_name', 'preference_rating', 'is_allowing_for_assistance', 'projection_start', 'projection_end', 'status', 'value', 'pct_phase3', 'pct_phase4', 'pct_phase5', 'description', 'id', 'datacollectionperiod', 'datacollection', 'scenario', 'geographic_unit', 'datasourceorganization', 'datasourcedocument', 'dataseries', 'dataseries_name', 'specialization_type', 'dataseries_specialization_type', 'data_usage_policy', 'created', 'modified', 'status_changed', 'collection_status', 'collection_status_changed', 'collection_schedule', 'reporting_date']\n",
      "Available country codes and names:\n",
      "AF: Afghanistan\n",
      "AO: Angola\n",
      "BF: Burkina Faso\n",
      "BI: Burundi\n",
      "CD: Congo, The Democratic Republic of the\n",
      "CF: Central African Republic\n",
      "CM: Cameroon\n",
      "ET: Ethiopia\n",
      "GT: Guatemala\n",
      "HN: Honduras\n",
      "HT: Haiti\n",
      "KE: Kenya\n",
      "LS: Lesotho\n",
      "MG: Madagascar\n",
      "ML: Mali\n",
      "MR: Mauritania\n",
      "MW: Malawi\n",
      "MZ: Mozambique\n",
      "NE: Niger\n",
      "NG: Nigeria\n",
      "NI: Nicaragua\n",
      "RW: Rwanda\n",
      "SD: Sudan\n",
      "SO: Somalia\n",
      "SS: South Sudan\n",
      "SV: El Salvador\n",
      "TD: Chad\n",
      "UG: Uganda\n",
      "VE: Venezuela, Bolivarian Republic of\n",
      "YE: Yemen\n",
      "ZW: Zimbabwe\n",
      "You selected the following countries:\n",
      "ET: Ethiopia\n",
      "Skipping IPC completeness graphic generation.\n",
      "\n",
      "Process Selection Options:\n",
      "\n",
      "1: - First checks for rows meeting both a proportional area threshold and a critical value.\n",
      "  - Assigns the maximum value from the filtered rows if conditions are met.\n",
      "  - Otherwise, calculates a fallback weighted sum of values by proportional area.\n",
      "\n",
      "2: - Assigns the value that occupies the largest proportion of area within each `pg_id`.\n",
      "  - Uses the row with the highest `Proportional_area` as the dominant value.\n",
      "\n",
      "3: - Multiplies each row’s value by its proportional area to compute a weighted value.\n",
      "  - Groups data by `pg_id` and sums the weighted values to produce a dissolved value.\n",
      "\n",
      "4: - Assigns the maximum value if its proportional area exceeds a specified threshold.\n",
      "  - If no rows meet the threshold, calculates a fallback weighted sum of values by proportional area.\n",
      "\n",
      "5: - Uses the row with the greatest population proportion within each `pg_id`.\n",
      "  - Combines population and area proportions to calculate weighted values, summing these at the `pg_id` level.\n",
      "\n",
      "6: - Applies custom weights to population and area proportions based on population percentiles.\n",
      "  - Calculates a weighted value for each row and aggregates by `pg_id`. (default)\n",
      "\n",
      "Please select a process: 1, 2, 3, 4, 5, or 6 (default: 6)\n",
      "Type 'q' to quit the process.\n",
      "Processing country: ET (Ethiopia)\n",
      "The API call constructed: https://fdw.fews.net/api/feature/?format=geojson&fields=with_attributes&country_code=ET&unit_type=idp_camp&unit_type=livelihood_zone&unit_type=national_park&unit_type=fsc_admin&unit_type=fsc_admin_lhz&unit_type=fsc_lhz&unit_type=fsc_rm_admin\n",
      "\n",
      "Data downloaded and saved as ipc_data.csv to folder Data/Generated/ipc_data.csv\n",
      "\n",
      "Merged DataFrame:\n",
      "             fnid          scenario_name  value reporting_date  \\\n",
      "0  ET2023C1020203      Current Situation    4.0     2023-10-01   \n",
      "1  ET2021C1020203      Current Situation    4.0     2022-06-01   \n",
      "2  ET2021C1020203  Near Term Projection     4.0     2022-06-01   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((39.91100 13.35300, 39.91052 13.33948...  \n",
      "1  POLYGON ((40.03742 13.56128, 40.03867 13.56068...  \n",
      "2  POLYGON ((40.03742 13.56128, 40.03867 13.56068...  \n",
      "\n",
      "Unmatched FNIDs:\n",
      "Empty DataFrame\n",
      "Columns: [fnid, scenario_name, value, reporting_date]\n",
      "Index: []\n",
      "\n",
      "Completeness of the merge: 100.00%\n",
      "Unmatched FNID count: 0\n",
      "Skipping IPC historical maps generation.\n",
      "['2022-06-01', '2022-10-01', '2023-02-01', '2023-06-01', '2023-10-01']\n",
      "Available dates:\n",
      "1. 2022-06-01\n",
      "2. 2022-10-01\n",
      "3. 2023-02-01\n",
      "4. 2023-06-01\n",
      "5. 2023-10-01\n",
      "\n",
      "Enter one of the following options:\n",
      "1. Type a specific date from the above list.\n",
      "2. Type 'All' to process all dates.\n",
      "3. Type 'Quit' to exit the program.\n",
      "\n",
      "Processing date: 2022-06-01\n",
      "\n",
      "Reprojecting GeoDataFrame to a projected CRS (UTM).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/Documents/FEWSnet/Utils/perform_intersection.py:24: UserWarning: `keep_geom_type=True` in overlay resulted in 6 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  intersected_gdf = gpd.overlay(gdf1, gdf2, how='intersection')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading shapefile from: /Users/gbenz/Documents/FEWSnet/Data/Processed/extent_shapefile/pg_viewser_extent.shp.\n",
      "We will use this geodataframe to define an extent that matches global pg_ids to those located in Ethiopia.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/miniforge3/envs/viewser/lib/python3.11/site-packages/ingester3/scratch.py:31: UserWarning: No database connection! Will try to use cache for read-only ops as much as I can\n",
      "  warnings.warn(\"No database connection! Will try to use cache for read-only ops as much as I can\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: (psycopg2.OperationalError) could not translate host name \"gjoll.muspelheim.local\" to address: nodename nor servname provided, or not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
      "\n",
      "Fields in DF2\n",
      "['pg_id', 'lat', 'long', 'geometry', 'dissolved_value', 'processing_date', 'country_code']\n",
      "\n",
      "Fields in DF1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbenz/Documents/FEWSnet/tests.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gbenz/Documents/FEWSnet/tests.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gbenz/Documents/FEWSnet/tests.ipynb#W4sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFields in DF1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gbenz/Documents/FEWSnet/tests.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39;49m(gpd_country_extent_df))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gbenz/Documents/FEWSnet/tests.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gbenz/Documents/FEWSnet/tests.ipynb#W4sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m country_joined\u001b[39m=\u001b[39mleft_join_geodataframes(gpd_country_extent_df, result_gdf)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if pg_shapefile_exists():\n",
    "    print(\"The PG reference shapefile has already been produced!\")\n",
    "else:\n",
    "    print(\n",
    "    \"The PG Shapefile does not exist. This process must be completed before integrating FEWSNET data.\\n\"\n",
    "    \"I will produce and save the PG reference file now. Once complete, it can be accessed in Data/Processed/extent_shapefile/\"\n",
    "    )\n",
    "    \n",
    "    provide_reference_frame()\n",
    "\n",
    "ipc_classification = select_ipc_classification()\n",
    "s, e = get_date_range()\n",
    "ipc = construct_ipc_api_url(s, e, ipc_classification)\n",
    "print(list(ipc))\n",
    "#Print all country_codes:\n",
    "#country_code_list = sorted(ipc['country_code'].unique().tolist())\n",
    "\n",
    "ipc_country, selected_country_codes = select_country_codes(ipc)\n",
    "\n",
    "endyear = e.split('-')[0]\n",
    "\n",
    "icp_country_result = plot_fewsnet_scenario_coverage(ipc, endyear)\n",
    "\n",
    "process_selection = get_process_selection()\n",
    "\n",
    "all_country_results = []\n",
    "\n",
    "for country_code in selected_country_codes:\n",
    "\n",
    "    # identify country associated with country code:\n",
    "\n",
    "    country_name = ipc.loc[ipc['country_code'] == country_code, 'country'].iloc[0]\n",
    "\n",
    "    print(f\"Processing country: {country_code} ({country_name})\")\n",
    "\n",
    "    boundaries = construct_boundary_api_url(country_code)\n",
    "\n",
    "    ipc_filtered, spatial_filtered = desirable_attributes(ipc_country, boundaries)\n",
    "\n",
    "    merge, unmatched = merge_ipc_with_boundaries(ipc_filtered, spatial_filtered)\n",
    "\n",
    "    evaluate_merge_completness(merge, ipc_filtered, unmatched)\n",
    "\n",
    "    scenario = define_scenario(merge)\n",
    "\n",
    "    merged_df_current = gpd.GeoDataFrame(scenario, geometry='geometry')\n",
    "\n",
    "    plot_historical_ipc(merged_df_current)\n",
    "\n",
    "    result_dfs = []\n",
    "\n",
    "    # Get unique dates from the dataset\n",
    "    dataset_dates = sorted(merged_df_current['reporting_date'].unique().tolist())\n",
    "    print(dataset_dates)\n",
    "\n",
    "    dates_to_process = get_dates_to_process(dataset_dates)\n",
    "\n",
    "    # Loop through each date in the dataset_dates\n",
    "    for i, processing_date in enumerate(dates_to_process):  # Enumerate gives both index and value\n",
    "        print()\n",
    "        print(f\"Processing date: {processing_date}\")\n",
    "        print()\n",
    "\n",
    "        # Extract the year from the current processing date\n",
    "        year = processing_date.split('-')[0]\n",
    "        year_int = int(processing_date.split('-')[0])\n",
    "\n",
    "        # Filter the dataset for the current date\n",
    "        merged_df_current_lim = merged_df_current[merged_df_current['reporting_date'] == processing_date]\n",
    "\n",
    "        # Generate a buffered envelope for the filtered dataset\n",
    "        envelope_gdf_buffered = envelope_buffer(merged_df_current_lim, distance=25000)\n",
    "\n",
    "        # Ensure only valid geometry types\n",
    "        merged_df_current_lim = merged_df_current_lim[merged_df_current_lim.geom_type.isin(['Polygon', 'MultiPolygon'])]\n",
    "\n",
    "        # Perform the intersection\n",
    "        intersected_gdf = intersect(merged_df_current_lim)\n",
    "\n",
    "        # Ensure area attributes are defined\n",
    "        intersected_gdf = define_area_attributes(intersected_gdf)\n",
    "\n",
    "        # Check if the selected process requires population data\n",
    "        if process_selection in [5, 6]:\n",
    "            intersected_gdf = engineer_population_attributes(year, country_code, intersected_gdf, envelope_gdf_buffered)\n",
    "\n",
    "        # Define the process and generate the result\n",
    "        result = define_process(process_selection, intersected_gdf)\n",
    "\n",
    "        # Rejoin the result to the priogrid\n",
    "        # Applies the data frame containing a pg attribute to the spatial pg extent\n",
    "        result_gdf = rejoin_to_pg(result)\n",
    "        \n",
    "        # Add processing_date and country_code fields to result_gdf\n",
    "        result_gdf['processing_date'] = processing_date\n",
    "        result_gdf['country_code'] = country_code\n",
    "\n",
    "        # Trim results to PG (viewser defined) country extent\n",
    "        gpd_country_extent_df = create_country_geodataframe(country_name, year_int, shapefile_path=None)\n",
    "        \n",
    "        print()\n",
    "        print('Fields in DF2')\n",
    "        print(list(result_gdf))\n",
    "        print()\n",
    "        print('Fields in DF1')\n",
    "        print(list(gpd_country_extent_df))\n",
    "        print()\n",
    "\n",
    "        country_joined=left_join_geodataframes(gpd_country_extent_df, result_gdf)\n",
    "\n",
    "        # Append the result_gdf to the list\n",
    "        result_dfs.append(country_joined)\n",
    "\n",
    "        print(f\"Completed processing for {processing_date}. Result saved.\")\n",
    "\n",
    "    # Concatenate the results for the current country\n",
    "    country_result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "\n",
    "    # Check if 'final_weighted_value' exists before renaming\n",
    "    if 'final_weighted_value' in country_result_df.columns:\n",
    "        country_result_df = country_result_df.rename(columns={'final_weighted_value': 'IPC_value'})\n",
    "\n",
    "    # Check if 'original_sum_value' exists before dropping\n",
    "    if 'original_sum_value' in country_result_df.columns:\n",
    "        country_result_df = country_result_df.drop(columns=['original_sum_value'])\n",
    "\n",
    "\n",
    "    # Append to the all-country results list\n",
    "    all_country_results.append(country_result_df)\n",
    "\n",
    "# Concatenate results for all countries\n",
    "final_result_df = pd.concat(all_country_results, ignore_index=True)\n",
    "\n",
    "# Naming Conventions:\n",
    "naming_conventions = apply_naming_convention(dates_to_process, selected_country_codes, process_selection)\n",
    "\n",
    "csv_path_with_naming = csv_path.with_name(csv_path.name + naming_conventions + '.csv')\n",
    "\n",
    "# The final_result_df now contains data for all countries and dates\n",
    "final_result_df.to_csv(csv_path_with_naming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ethiopia'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shapefile from: /Users/gbenz/Documents/FEWSnet/Data/Processed/extent_shapefile/pg_viewser_extent.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/miniforge3/envs/viewser/lib/python3.11/site-packages/ingester3/scratch.py:31: UserWarning: No database connection! Will try to use cache for read-only ops as much as I can\n",
      "  warnings.warn(\"No database connection! Will try to use cache for read-only ops as much as I can\")\n",
      "/Users/gbenz/miniforge3/envs/viewser/lib/python3.11/site-packages/ingester3/scratch.py:31: UserWarning: No database connection! Will try to use cache for read-only ops as much as I can\n",
      "  warnings.warn(\"No database connection! Will try to use cache for read-only ops as much as I can\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available countries: ['Algeria', 'Angola', 'Benin', 'Botswana', 'Burkina Faso', 'Burundi', 'Cameroon', 'Cape Verde', 'Central African Republic', 'Chad', 'Comoros', 'Congo', 'Congo, DRC', \"Cote d'Ivoire\", 'Djibouti', 'Egypt', 'Equatorial Guinea', 'Eritrea', 'Ethiopia', 'Gabon', 'Ghana', 'Guinea', 'Guinea-Bissau', 'Kenya', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Mali', 'Mauritania', 'Mauritius', 'Morocco', 'Mozambique', 'Namibia', 'Niger', 'Nigeria', 'Rwanda', 'Sao Tome and Principe', 'Senegal', 'Seychelles', 'Sierra Leone', 'Somalia', 'South Africa', 'South Sudan', 'Sudan', 'Swaziland', 'Tanzania', 'The Gambia', 'Togo', 'Tunisia', 'Uganda', 'Zambia', 'Zimbabwe']\n",
      "Filtered data for Ethiopia in 2022:\n",
      "      c_id  year_id      name\n",
      "4374    57     2022  Ethiopia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/miniforge3/envs/viewser/lib/python3.11/site-packages/ingester3/scratch.py:31: UserWarning: No database connection! Will try to use cache for read-only ops as much as I can\n",
      "  warnings.warn(\"No database connection! Will try to use cache for read-only ops as much as I can\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered priogrid IDs:\n",
      "['c_id', 'year_id', 'name', 'pg_id']\n",
      "Conversion to GeoDataFrame successful!\n"
     ]
    }
   ],
   "source": [
    "from Utils.pg_country_extent import create_country_geodataframe\n",
    "\n",
    "country_name = 'Ethiopia'\n",
    "year = 2022\n",
    "\n",
    "gpd_df = create_country_geodataframe(country_name, year, shapefile_path=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "4. Trim to Country PG extent \n",
    "4. provide a name, informed by the parameters, to ingest into viewser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viewser import Queryset, Column\n",
    "import pandas as pd\n",
    "\n",
    "from ingester3.extensions import *\n",
    "\n",
    "def give_primary_frame(queryset_name, cm_queryset, start, end):\n",
    "\n",
    "    \"\"\"\"\n",
    "    1. retrieves base queryset\n",
    "    2.assigns a year field from Ingester3 functions\n",
    "    3. trims to defined intervals\n",
    "    4. sum fatalities\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#Here is the queryset used:\n",
    "    \n",
    "    # (Queryset('Fatalities_fao_pgm','priogrid_month')\n",
    "    # .with_column(Column('country_name', from_loa='country', from_column='name')\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('C_start_year', from_loa='country', from_column='gwsyear')\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('C_end_year', from_loa='country', from_column='gweyear')\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('pop_gpw_sum', from_loa='priogrid_year', from_column='pop_gpw_sum')\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('ged_sb', from_loa='priogrid_month', from_column='ged_sb_best_sum_nokgi')\n",
    "    #     .transform.missing.replace_na()\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('ged_ns', from_loa='priogrid_month', from_column='ged_ns_best_sum_nokgi')\n",
    "    #     .transform.missing.replace_na()\n",
    "    #     )\n",
    "\n",
    "    # .with_column(Column('ged_os', from_loa='priogrid_month', from_column='ged_os_best_sum_nokgi')\n",
    "    #     .transform.missing.replace_na()\n",
    "    #     )\n",
    "\n",
    "    # )\n",
    "\n",
    "    queryset_base_PG= (Queryset(queryset_name, ''))\n",
    "    df = queryset_base_PG.fetch()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    queryset_cm= (Queryset(cm_queryset, ''))\n",
    "    cm_properties = queryset_cm.fetch()\n",
    "    cm_properties = cm_properties.reset_index()    \n",
    "\n",
    "    df['year'] = df.m.year\n",
    "\n",
    "    df_trimmed = df[(df['year'] > start) & (df['year'] < end)]\n",
    "    df_trimmed['fatalities_sum'] = df_trimmed['ged_sb'] + df_trimmed['ged_ns'] + df_trimmed['ged_os']\n",
    "\n",
    "\n",
    "    merged_cid = pd.merge(df_trimmed, cm_properties[['month_id', 'country_name', 'country_id']], on=['month_id', 'country_name'], how='left')\n",
    "    #print(merged_cid)\n",
    "    df_pg = merged_cid\n",
    "\n",
    "    df_pg = df_pg.rename(columns={'priogrid_gid':'pg_id'})\n",
    "    #df_pg['year_id'] = df_pg['month_id'].apply(views_month_id_to_year)\n",
    "\n",
    "    print(list(df_pg))\n",
    "    return(df_pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "Clip processessing to the extent defined in viewser system \n",
    "\n",
    "1. Get country name from the country code selection earlier\n",
    "2. apply current year parameter\n",
    "\n",
    "\n",
    "parameters:\n",
    "- pcd dataframe (this should be constructed with an option parameter to designate a speicifc destination)\n",
    "- year \n",
    "- country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as pgd \n",
    "from ingester3.extensions import *\n",
    "\n",
    "gpd_df = gpd.read_file('Data/Processed/extent_shapefile/pg_viewser_extent.shp')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_country_year = pd.DataFrame.cy.new_africa(max_year=2020) # year + 1\n",
    "new_country_year['name'] = new_country_year.c.name\n",
    "print(new_country_year)\n",
    "\n",
    "countries = sorted(pd.unique(new_country_year['name'].tolist()))\n",
    "print(countries)\n",
    "\n",
    "\n",
    "filtered_df = new_country_year[(new_country_year['name'] == 'Ethiopia') & (new_country_year['year_id'] == 2020)]\n",
    "print(filtered_df)\n",
    "\n",
    "filtered_df_pg=filtered_df.cy.pg_id\n",
    "\n",
    "print(list(filtered_df_pg))\n",
    "\n",
    "merged_df = pd.merge(filtered_df_pg, gpd_df, on='pg_id', how='inner')  # Default join is inner\n",
    "print(list(merged_df))\n",
    "\n",
    "\n",
    "# Assuming `df` is your DataFrame\n",
    "if 'geometry' in merged_df.columns:\n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(merged_df, geometry=merged_df['geometry'])\n",
    "\n",
    "    # Ensure CRS is set\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(\"EPSG:4326\", inplace=True)  # Assuming WGS84 CRS (latitude/longitude)\n",
    "\n",
    "    print(\"Conversion to GeoDataFrame successful!\")\n",
    "    print(gdf.head())  # Display the first few rows\n",
    "else:\n",
    "    print(\"Error: 'geometry' column not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'merged_df' is your GeoDataFrame\n",
    "gdf.plot(\n",
    "    edgecolor='black',  # Set edge color to black\n",
    "    linewidth=0.5,      # Set line width for grid edges\n",
    "    facecolor='none',   # Make polygons transparent\n",
    "    figsize=(10, 6)     # Adjust figure size\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Polygon Grids\", fontsize=16)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how easy it is to convert a CY dataframe in a PGY dataframe\n",
    "gpd['c_id'] = gpd.pgy.c_id\n",
    "\n",
    "c_d.cy.pg_id\n",
    "gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from ingester3 import *\n",
    "df_with_cid = pd.DataFrame.c.from_pg(intersected_gdf, pg_col='pg_id')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
